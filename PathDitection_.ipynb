{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SantayogithubIT/WalkableArea/blob/main/PathDitection_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4q-zlgKWdVee"
      },
      "outputs": [],
      "source": [
        "!pip install ultralytics opencv-python pyttsx3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIQDGNCIdiI_"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "from google.colab.patches import cv2_imshow\n",
        "import pyttsx3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nwncWdAdljV"
      },
      "outputs": [],
      "source": [
        "model = YOLO(\"yolov8n-seg.pt\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5o1cNVhnH8r"
      },
      "outputs": [],
      "source": [
        "!pip install gTTS\n",
        "from gtts import gTTS\n",
        "from IPython.display import Audio, display\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGOBTH9PnNs0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def speak(text, filename=\"voice.mp3\"):\n",
        "    tts = gTTS(text=text, lang='en')\n",
        "    tts.save(filename)\n",
        "    display(Audio(filename, autoplay=True))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzNL03mWnSZC"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import cv2\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "from gtts import gTTS\n",
        "from IPython.display import Audio, display\n",
        "from google.colab.patches import cv2_imshow\n",
        "import time\n",
        "\n",
        "# ---------------------------\n",
        "# TEXT-TO-SPEECH FUNCTION\n",
        "# ---------------------------\n",
        "def speak(text, filename=\"voice.mp3\"):\n",
        "    print(\"[AUDIO]:\", text)\n",
        "    tts = gTTS(text=text, lang='en')\n",
        "    tts.save(filename)\n",
        "    display(Audio(filename, autoplay=True))\n",
        "\n",
        "# ---------------------------\n",
        "# Load YOLO Segmentation Model\n",
        "# ---------------------------\n",
        "model = YOLO(\"yolov8n-seg.pt\")\n",
        "\n",
        "# Upload video\n",
        "uploaded = files.upload()\n",
        "video_path = list(uploaded.keys())[0]\n",
        "\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# ---------------------------\n",
        "# Helper function\n",
        "# ---------------------------\n",
        "def get_direction(center_x, width):\n",
        "    left = width // 3\n",
        "    right = 2 * width // 3\n",
        "\n",
        "    if center_x < left:\n",
        "        return \"Move left\"\n",
        "    elif center_x > right:\n",
        "        return \"Move right\"\n",
        "    else:\n",
        "        return \"Move forward\"\n",
        "\n",
        "last_direction = \"\"\n",
        "last_obstacle_time = 0\n",
        "last_speak_time = 0\n",
        "\n",
        "OBSTACLE_CLASSES = [0, 1, 2, 3]  # person, bicycle, car, motorcycle (adjust as needed)\n",
        "\n",
        "# ---------------------------\n",
        "# MAIN PROCESS LOOP\n",
        "# ---------------------------\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    frame_w = frame.shape[1]\n",
        "    frame_h = frame.shape[0]\n",
        "\n",
        "    results = model(frame, verbose=False)[0]\n",
        "\n",
        "    walk_center_x = None\n",
        "    mask_found = False\n",
        "\n",
        "    # ---------------------------\n",
        "    # MASK PROCESSING\n",
        "    # ---------------------------\n",
        "    if results.masks is not None:\n",
        "        masks = results.masks.data.cpu().numpy()\n",
        "\n",
        "        # Pick LARGEST mask\n",
        "        areas = [np.sum(m) for m in masks]\n",
        "        largest_idx = int(np.argmax(areas))\n",
        "\n",
        "        mask = masks[largest_idx]\n",
        "        mask = cv2.resize(mask, (frame_w, frame_h))\n",
        "\n",
        "        mask_found = True\n",
        "\n",
        "        # Get walkable center\n",
        "        ys, xs = np.where(mask > 0.5)\n",
        "        if len(xs) > 0:\n",
        "            walk_center_x = int(np.mean(xs))\n",
        "\n",
        "        # Overlay mask (green transparent)\n",
        "        overlay = frame.copy()\n",
        "        overlay[mask > 0.5] = (0, 255, 0)\n",
        "        frame = cv2.addWeighted(frame, 0.7, overlay, 0.3, 0)\n",
        "\n",
        "    # ---------------------------\n",
        "    # OBSTACLE DETECTION\n",
        "    # ---------------------------\n",
        "    obstacle_detected = False\n",
        "\n",
        "    for box in results.boxes:\n",
        "        cls = int(box.cls[0])\n",
        "        conf = float(box.conf[0])\n",
        "\n",
        "        if cls in OBSTACLE_CLASSES and conf > 0.5:\n",
        "            obstacle_detected = True\n",
        "\n",
        "            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0,0,255), 2)\n",
        "            cv2.putText(frame, \"Obstacle\", (x1, y1-5),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,0,255), 2)\n",
        "\n",
        "    # Speak only once every 2 seconds\n",
        "    if obstacle_detected and time.time() - last_obstacle_time > 2:\n",
        "        speak(\"Obstacle ahead!\")\n",
        "        last_obstacle_time = time.time()\n",
        "\n",
        "    # ---------------------------\n",
        "    # DIRECTION GUIDANCE\n",
        "    # ---------------------------\n",
        "    if walk_center_x is not None:\n",
        "        direction = get_direction(walk_center_x, frame_w)\n",
        "\n",
        "        cv2.circle(frame, (walk_center_x, frame_h//2), 8, (255, 0, 0), -1)\n",
        "        cv2.putText(frame, direction, (50,50),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2)\n",
        "\n",
        "        # Speak only if direction changed\n",
        "        if direction != last_direction and time.time() - last_speak_time > 1.5:\n",
        "            speak(direction)\n",
        "            last_direction = direction\n",
        "            last_speak_time = time.time()\n",
        "\n",
        "    else:\n",
        "        cv2.putText(frame, \"No walkable path detected\", (50, 50),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "\n",
        "    # ---------------------------\n",
        "    # SHOW OUTPUT FRAME\n",
        "    # ---------------------------\n",
        "    cv2_imshow(frame)\n",
        "\n",
        "cap.release()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WYHv58DdnIf"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "video_path = list(uploaded.keys())[0]\n",
        "\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    results = model(frame)\n",
        "    annotated = results[0].plot()    # YOLO segmentation overlay\n",
        "\n",
        "    cv2_imshow(annotated)\n",
        "\n",
        "cap.release()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLeOJGdYeBIA"
      },
      "outputs": [],
      "source": [
        "!pip install ultralytics gTTS\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "from gtts import gTTS\n",
        "from IPython.display import Audio, display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "# -------------------------------\n",
        "# gTTS Speak Function\n",
        "# -------------------------------\n",
        "def speak(text, filename=\"direction.mp3\"):\n",
        "    tts = gTTS(text=text, lang='en')\n",
        "    tts.save(filename)\n",
        "    display(Audio(filename, autoplay=True))\n",
        "\n",
        "# -------------------------------\n",
        "# Webcam Capture (JavaScript)\n",
        "# -------------------------------\n",
        "def capture_frame():\n",
        "    js_code = Javascript('''\n",
        "        async function takePhoto() {\n",
        "          const video = document.createElement('video');\n",
        "          document.body.appendChild(video);\n",
        "          const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "          video.srcObject = stream;\n",
        "          await video.play();\n",
        "\n",
        "          const canvas = document.createElement('canvas');\n",
        "          canvas.width = video.videoWidth;\n",
        "          canvas.height = video.videoHeight;\n",
        "          const context = canvas.getContext('2d');\n",
        "          context.drawImage(video, 0, 0);\n",
        "\n",
        "          stream.getTracks().forEach(t => t.stop());\n",
        "          document.body.removeChild(video);\n",
        "\n",
        "          return canvas.toDataURL('image/jpeg');\n",
        "        }\n",
        "        takePhoto();\n",
        "    ''')\n",
        "\n",
        "    display(js_code)\n",
        "    data = eval_js('takePhoto()')\n",
        "    img_bytes = b64decode(data.split(',')[1])\n",
        "\n",
        "    with open(\"frame.jpg\", \"wb\") as f:\n",
        "        f.write(img_bytes)\n",
        "\n",
        "    return cv2.imread(\"frame.jpg\")\n",
        "\n",
        "# -------------------------------\n",
        "# Load YOLO Segmentation Model\n",
        "# -------------------------------\n",
        "model = YOLO(\"yolov8n-seg.pt\")\n",
        "\n",
        "\n",
        "last_direction = \"\"  # avoid repeated speaking\n",
        "\n",
        "def get_direction(cx, width):\n",
        "    if cx < width / 3:\n",
        "        return \"Move left\"\n",
        "    elif cx > 2 * width / 3:\n",
        "        return \"Move right\"\n",
        "    else:\n",
        "        return \"Move forward\"\n",
        "\n",
        "# -------------------------------\n",
        "# MAIN LOOP (Capture ‚Üí Detect ‚Üí Speak)\n",
        "# -------------------------------\n",
        "while True:\n",
        "    print(\"üì∏ Capturing frame... Look at your webcam.\")\n",
        "    frame = capture_frame()\n",
        "\n",
        "    if frame is None:\n",
        "        print(\"‚ùå No frame captured.\")\n",
        "        break\n",
        "\n",
        "    results = model(frame)[0]\n",
        "\n",
        "    walk_center_x = None\n",
        "\n",
        "    if results.masks is not None:\n",
        "        masks = results.masks.data.cpu().numpy()\n",
        "\n",
        "        # Pick largest mask (walkable region)\n",
        "        areas = [np.sum(mask) for mask in masks]\n",
        "        idx = int(np.argmax(areas))\n",
        "\n",
        "        mask = cv2.resize(masks[idx], (frame.shape[1], frame.shape[0]))\n",
        "\n",
        "        ys, xs = np.where(mask > 0.5)\n",
        "        if len(xs) > 0:\n",
        "            walk_center_x = int(np.mean(xs))\n",
        "\n",
        "    # -----------------------\n",
        "    # Decision + Voice Output\n",
        "    # -----------------------\n",
        "    if walk_center_x is not None:\n",
        "        frame_w = frame.shape[1]\n",
        "        direction = get_direction(walk_center_x, frame_w)\n",
        "\n",
        "        print(\"‚û°Ô∏è Direction:\", direction)\n",
        "\n",
        "        if direction != last_direction:\n",
        "            speak(direction)\n",
        "            last_direction = direction\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No walkable area detected\")\n",
        "        speak(\"No walkable area detected\")\n",
        "\n",
        "    # -----------------------\n",
        "    # Show annotated image\n",
        "    # -----------------------\n",
        "    annotated = results.plot()\n",
        "    cv2_imshow(annotated)\n",
        "\n",
        "    # Ask user if they want to continue\n",
        "    cont = input(\"Capture another frame? (y/n): \")\n",
        "    if cont.lower() != \"y\":\n",
        "        break\n",
        "\n",
        "print(\"‚úÖ Finished.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "klGY_w01KAap"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}